---
layout: post
title: "Ideas regarding PhD topic"
date: 2016-04-02
comments: true
archive: true
notes: true
tags: notes
excerpt: Thoughts and notes regarding PhD topics
---

<p>
<b>Ideas:</b>
</p>

<p>
Kernels run out of memory while NN's are compact function classes
providing a trade off between storage vs training time computation.
</p>

<p>
Exploit the trades of both of the methodls and combine them for
nonparametric statistical test, generative modes, message passing,
bandit algorithms and other things that need good statistical analysis
and flexible models.
</p>

<p>
<code>Problem which still remains to be solved, is how to incorporate model</code>
<code>decompositions efficiently into deep learning?</code>
</p>


<p>
<code>deep learning + spectral methods ==&gt; How to combine them?</code>
</p>

<p>
<code>This can be done e.g. using some of the objective functions for from</code>
<code>graphical models .e.g. Conditional Random Fields, Structured loss,=
=anything similar</code>
</p>

<p>
<b>Differences between graphical models and deep learning:</b>
</p>

<ul class="org-ul">
<li><b>graphical models</b> are good if you've got a lot of variables and
want to know how they depend on each other. Explains a lot about
clustering, topic models, Bayesian nonparametrics, causality and
message passing</li>

<li><b>deep learning</b> is about understanding how to use them efficiently
and which are the limitations. <i>Statistical learning theory</i> in this
case is necessary to prove theorems about whether your algorithm
works or not. You want to have a guarantee for what you're doing
won't go wrong but you don't really want to use the theoremsfor
parameter tuning.</li>
</ul>
