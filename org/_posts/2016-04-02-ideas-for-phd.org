#+STARTUP: showall indent
#+STARTUP: hidestars
#+BEGIN_HTML
---
layout: post
title: "Ideas regarding PhD topic"
date: 2016-04-02
comments: true
archive: true
notes: true
tags: notes
excerpt: Thoughts and notes regarding PhD topics
---
#+END_HTML

*Ideas:*

Kernels run out of memory while NN's are compact function classes
providing a trade off between storage vs training time computation.

Exploit the trades of both of the methodls and combine them for
nonparametric statistical test, generative modes, message passing,
bandit algorithms and other things that need good statistical analysis
and flexible models.

=Problem which still remains to be solved, is how to incorporate model=
=decompositions efficiently into deep learning?=


=deep learning + spectral methods ==> How to combine them?=

=This can be done e.g. using some of the objective functions for from=
=graphical models .e.g. Conditional Random Fields, Structured loss,=
=anything similar=

*Differences between graphical models and deep learning:*

- *graphical models* are good if you've got a lot of variables and
  want to know how they depend on each other. Explains a lot about
  clustering, topic models, Bayesian nonparametrics, causality and
  message passing


- *deep learning* is about understanding how to use them efficiently
  and which are the limitations. /Statistical learning theory/ in this
  case is necessary to prove theorems about whether your algorithm
  works or not. You want to have a guarantee for what you're doing
  won't go wrong but you don't really want to use the theoremsfor
  parameter tuning.


*LSTM's* are latent variable auto-regressive models with some fine
 tuning to deal with vanishing gradients

Adverserial Environments hard to handle


*Mastering Algorithm* by P.Domingos:

At the time of writing it has been identified that there are 5
different tribes, schools/paradigms regarding machine learning related
to the way that each school or technique uses they preferred
methodology or algorithm inside the machine learning community.

How do computers discover new knowledge?

1. Fill in gaps in existing knowledge
2. Emulate the brain
3. Simulate evolution
4. Systematically reduce uncertainty
5. Notice similarities between old and new


| Tribe          | Origins              | Master Algorithm        | People |
|----------------+----------------------+-------------------------+----|
|                |                      |                         | <2> |
| Symbolists     | Logic, philosophy    | Inverse deduction       | Tom Mitchel, Steve Muggleton, Ross Quinlan |
| Connectionists | Neuroscience         | Backpropagation         | LeCun, Hinton, Bengio |
| Evolutionaries | Evolutionary Biology | Genetic programming     | John Koza, John Holland, Hod Lipson |
| Bayesians      | Statistics           | Probabilistic inference | David Heckerman, Judea Pearl, Michael Jordan |
| Analogizers    | Psychology           | Kernel machines         | Peter Hart, V.Vapnik, Douglas Hofstadter |


Putting pieces together:

- Representation
 - Probabilistic logic (e.g. Markov logic networks)
 - Weighted formulas --> Distriubtion over states

- Evaluation
 - Posterior probability
 - User defined objective function

- Optimization
 - Formula discovery: Genetic programming
 - Weight learning: Backpropagation

- Towards a universal learner
 - New ideas and tribes are needed ==> ?
