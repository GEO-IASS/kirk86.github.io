#+STARTUP: showall indent
#+STARTUP: hidestars
#+BEGIN_HTML
---
layout: post
title: "Ideas regarding PhD topic"
date: 2016-04-02
comments: true
archive: true
notes: true
tags: notes
excerpt: Thoughts and notes regarding PhD topics
---
#+END_HTML

*Ideas:*

Kernels run out of memory while NN's are compact function classes
providing a trade off between storage vs training time computation.

Exploit the trades of both of the methodls and combine them for
nonparametric statistical test, generative modes, message passing,
bandit algorithms and other things that need good statistical analysis
and flexible models.

=Problem which still remains to be solved, is how to incorporate model=
=decompositions efficiently into deep learning?=


=deep learning + spectral methods ==> How to combine them?=

=This can be done e.g. using some of the objective functions for from=
=graphical models .e.g. Conditional Random Fields, Structured loss,=
=anything similar=
